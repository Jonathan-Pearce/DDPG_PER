# Continuous Control With Deep Reinforcement Learning - DDPG with Prioritized Experience Replay

Deep Deterministic Policy Gradient (DDPG) is currently one of the most popular deep reinforcement learning algorithms for continuous control. Inspired by the Deep Q-network algorithm (DQN) that works with discrete action spaces, DDPG uses a replay buffer to stabilize Q-learning. It has been demonstrated that prioritized experience replay (PER) can improve the performance of DQN. We investigate whether prioritized experience replay can have a similar effect with a continuous control algorithm such as DDPG. In this project we have reproduced the DDPG algorithm, integrated prioritized experience replay with DDPG and evaluated both algorithms on two popular benchmarking tasks for continuous control methods. Our experiments show that prioritized experience replay can improve the performance of the DDPG algorithm.

For full project report please [click here](report.pdf) or go to report.pdf above
